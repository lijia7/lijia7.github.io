---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am Jiaqi Li, a current (2023.09-) master student in Artificial Intelligence at Huazhong University of Science and Technology (HUST), supervised by Prof. Zhiguo Cao. Prior to that, I received my B.S. degree from Huazhong University of Science and Technology in 2023. 

My current research interests are 3D vision, autonomous driving and diffusion-based depth estimation.


# ğŸ”¥ News
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ <a href='https://neurips.cc/virtual/2024/poster/93704'>SDDR</a> is accepted by **NeurIPS 2024**.
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ <a href='https://ieeexplore.ieee.org/document/10707178'>NVDS+</a>"NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth Estimation" is accepted by **TPAMI**. 
- *2024.05*: &nbsp;ğŸ‰ğŸ‰ Winner of two international prizes(**<a href='https://cvlai.net/ntire/2024'>NTIRE 2024</a>** and **<a href='https://sites.google.com/view/eccv24-tricky-workshop/home'>TRICKY 2024</a>**). One paper is accepted by **<a href='https://arxiv.org/abs/2408.06083'>ECCV 2024 Workshop</a>**.
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ <a href='https://dlnext.acm.org/doi/abs/10.1145/3581783.3611807'>DADP</a> is accepted by **ACM MM 2023**.
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ <a href='https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html'>NVDS</a> is accepted by **ICCV 2023**. 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/acmmm2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Video Depth Stabilizer](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html)

Yiran Wang, Min Shi, **Jiaqi Li**, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian,  Guosheng Lin

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

- We propose a plug-and-play and bidirectional learning-based framework termed Neural Video Depth Stabilizer(NVDS), which can be directly adapted to different single-image depth predictors to remove flickers.
- We propose VDW dataset, which is currently the largest video depth dataset in the wild with the most diverse video scenes.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2023</div><img src='images/acmmm2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Diffusion-Augmented Depth Prediction with Sparse Annotations](https://dlnext.acm.org/doi/abs/10.1145/3581783.3611807)

**Jiaqi Li**, Yiran Wang, Zihao Huang, Jinghong Zheng, Ke Xian, Zhiguo Cao, Jianming Zhang

The depth annotations collected by LiDAR in autonomous driving scenarios are highly sparse, and it is difficult for the model to reconstruct a dense and complete depth map from this learning. The previous methods have poor robustness in challenging scenarios such as night, rain, and dazzling light, and cannot meet the practical applications. We propose a plug-and-play framework based on diffusion modeling and object-guided integrality loss to enhance global and local structural integrity, respectively.
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)
